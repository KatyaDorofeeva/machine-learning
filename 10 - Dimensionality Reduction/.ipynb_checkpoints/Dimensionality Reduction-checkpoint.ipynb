{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset(s) needed: MNIST (\"Modified National Institute of Standards and Technology\") dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load the MNIST dataset\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Q.1. Split the data into a training set and a test set (take the first 60,000 instances for training, and the remaining 10,000 for testing).\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#TODO\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = mnist.data / 255.0\n",
    "y = mnist.target\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=60000,train_size=10000, random_state=42)\n",
    "display(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Q.2. Train a Logistic Regression classifier on the dataset and see how long it takes.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 4.53s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "start_time = time.time()\n",
    "\n",
    "#TODO: Train the classifier\n",
    "log_clf.fit(X_train, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training took {:.2f}s\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Q.3. Evaluate the resulting model on the test set.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9018"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = log_clf.predict(X_test)\n",
    "log_clf.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Q.4. Use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Components:  151\n",
      "Total explained variance =  0.95\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(0.95)\n",
    "pca_result = pca.fit_transform(X_train)\n",
    " \n",
    "# sum = 0\n",
    "# for i,n in pca.explained_variance_ratio_ :\n",
    "#     sum = sum+n\n",
    "# print(sum)\n",
    "\n",
    "#print('Explained variation per principal component: {}'.format(pca.explained_variance_ratio_ ))\n",
    "print (\"Components: \", pca.n_components_)\n",
    "print (\"Total explained variance = \", round(pca.explained_variance_ratio_.sum(),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Q.5. Train a new Logistic Regression classifier on the reduced dataset and see how long it takes. Was training much faster? Explain your results.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 1.15s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "#TODO\n",
    "X_train151 = X_train[:,:151]\n",
    "X_test151 = X_test[:,:151]\n",
    "log_clf = LogisticRegression()\n",
    "start_time = time.time()\n",
    "\n",
    "#TODO: Train the classifier\n",
    "log_clf.fit(X_train151, y_train)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training took {:.2f}s\".format(end_time - start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Q.6. Evaluate the new classifier on the test set: how does it compare to the previous classifier? Discuss the speed / accuracy trade-off and in which case you'd prefer a very slight drop in model performance for a x-time speedup in training.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32893333333333336"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the faster it gets executed,the lower accuracy it porduces. The case for a slight drop in accuracy would be more preferred \n",
    "#when using the model on devices like TVs,when you would not want to wait hours for execution with the high accuracy\n",
    "\n",
    "log_clf.score(X_test151, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Q.7. Create a new text cell in your Notebook: Complete a 50-100 word summary \n",
    "    (or short description of your thinking in applying this week's learning to the solution) \n",
    "     of your experience in this assignment. Include:\n",
    "<br>                                                                    \n",
    "What was your incoming experience with this model, if any?\n",
    "what steps you took, what obstacles you encountered.\n",
    "how you link this exercise to real-world, machine learning problem-solving. (What steps were missing? What else do you need to learn?)\n",
    "This summary allows your instructor to know how you are doing and allot points for your effort in thinking and planning, and making connections to real-world work.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using dimensionality reduction techniques helps to reduce the number of features in your dataset without having to lose much information and keep (or improve) the model’s performance.\n",
    "#Here are some of the benefits of applying dimensionality reduction to a dataset:\n",
    "\n",
    "#Space required to store the data is reduced as the number of dimensions comes down\n",
    "##Less dimensions lead to less computation/training time\n",
    "#Some algorithms do not perform well when we have a large dimensions. So reducing these dimensions needs to happen for the algorithm to be useful\n",
    "#It takes care of multicollinearity by removing redundant features. For example, you have two variables – ‘time spent on treadmill in minutes’ and ‘calories burnt’. These variables are highly correlated as the more time you spend running on a treadmill, the more calories you will burn. Hence, there is no point in storing both as just one of them does what you require\n",
    "#It helps in visualizing data. it is very difficult to visualize data in higher dimensions so reducing our space to 2D or 3D may allow us to plot and observe patterns more clearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
